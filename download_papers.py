import os
import re
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from habanero import Crossref
from googletrans import Translator

# è®¾ç½®ä¿å­˜è·¯å¾„
SAVE_FOLDER = "D:/BaiduSyncdisk/å‚è€ƒæ–‡çŒ®"
os.makedirs(SAVE_FOLDER, exist_ok=True)

# è·å–æ–‡ç« å…ƒæ•°æ®
def get_metadata(doi):
    cr = Crossref()
    metadata = cr.works(ids=doi)
    item = metadata['message']

    year = item['issued']['date-parts'][0][0]
    first_author = item['author'][0]['family']
    journal = item['short-container-title'][0] if item['short-container-title'] else item['container-title'][0]
    title_en = item['title'][0]
    return year, first_author, journal, title_en

# ç¿»è¯‘è‹±æ–‡æ ‡é¢˜ä¸ºä¸­æ–‡
def translate_title(title_en):
    translator = Translator()
    try:
        translated = translator.translate(title_en, src='en', dest='zh-cn')
        return translated.text
    except:
        return title_en  # ç¿»è¯‘å¤±è´¥åˆ™ä¿ç•™è‹±æ–‡

# æ¸…ç†éæ³•æ–‡ä»¶åå­—ç¬¦
def sanitize_filename(s):
    return re.sub(r'[\\/*?:"<>|]', '', s)

# ä¸‹è½½ PDF æ–‡ä»¶
def download_pdf(pdf_url, filename):
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        r = requests.get(pdf_url, headers=headers, timeout=20)
        if 'pdf' not in r.headers.get('Content-Type', ''):
            print(f"âš ï¸ ä¸æ˜¯ PDF æ–‡ä»¶ï¼š{pdf_url}")
            return False
        save_path = os.path.join(SAVE_FOLDER, filename)
        with open(save_path, 'wb') as f:
            f.write(r.content)
        print(f"âœ… å·²ä¿å­˜ï¼š{save_path}")
        return True
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ï¼š{pdf_url}ï¼Œé”™è¯¯ï¼š{e}")
        return False

# ä»é¡µé¢ä¸­æå– DOI æˆ– PDF é“¾æ¥
def extract_doi_and_pdf(html, base_url):
    soup = BeautifulSoup(html, "html.parser")

    # å°è¯•æå– DOI
    doi_match = re.search(r'10.\d{4,9}/[-._;()/:A-Z0-9]+', html, re.IGNORECASE)
    doi = doi_match.group(0) if doi_match else None

    # å°è¯•æå– PDF é“¾æ¥
    pdf_url = None
    for a in soup.find_all('a', href=True):
        href = a['href']
        if ".pdf" in href.lower():
            pdf_url = urljoin(base_url, href)
            break

    return doi, pdf_url

# ä¸»å¤„ç†å‡½æ•°
def process_article_page(url):
    print(f"\nğŸ“˜ æ­£åœ¨å¤„ç†é¡µé¢ï¼š{url}")
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        response = requests.get(url, headers=headers, timeout=20)
        html = response.text
        base_url = response.url  # æœ€ç»ˆè·³è½¬åé¡µé¢

        doi, pdf_url = extract_doi_and_pdf(html, base_url)

        if not doi:
            print("âŒ æœªæå–åˆ° DOIï¼Œæ— æ³•è·å–å…ƒæ•°æ®ã€‚è·³è¿‡ã€‚")
            return

        # è·å–å…ƒæ•°æ®
        year, first_author, journal, title_en = get_metadata(doi)
        title_zh = translate_title(title_en)
        clean_title = sanitize_filename(title_zh)

        filename = f"{year}{first_author}-{journal}-{clean_title}.pdf"

        if not pdf_url:
            print("âš ï¸ æœªæ‰¾åˆ° PDF é“¾æ¥ï¼Œæ— æ³•ä¸‹è½½")
            return

        download_pdf(pdf_url, filename)

    except Exception as e:
        print(f"âŒ é¡µé¢å¤„ç†å¤±è´¥ï¼š{e}")

# ä¸»å…¥å£
if __name__ == "__main__":
    input_urls = [
        "https://www.nature.com/articles/s41524-025-01675-6",
     #   "https://www.nature.com/articles/s41586-020-2649-2",
     #   "https://science.org/doi/10.1126/science.abd4559",
        # æ›´å¤š HTML é¡µé¢é“¾æ¥...
    ]

    for url in input_urls:
        process_article_page(url)
        time.sleep(1)